{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e9c2662-7e16-46ff-a522-b75080b6d773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import logging\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "# Data processing imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Deep learning imports\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModel,\n",
    "    AutoModelForSequenceClassification\n",
    ")\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Vector database imports\n",
    "from qdrant_client import QdrantClient, models\n",
    "from qdrant_client.http.models import (\n",
    "    VectorParams, \n",
    "    Distance, \n",
    "    PointStruct, \n",
    "    Filter, \n",
    "    FieldCondition, \n",
    "    MatchValue\n",
    ")\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0681f03c-d973-444d-a937-357e2b86ce20",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad990c21-cec2-469d-814a-403d20297c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-12 17:00:41,362 - INFO - Model initialized successfully on cpu\n"
     ]
    }
   ],
   "source": [
    "def initialize_model(model_name: str = \"havocy28/VetBERT\") -> tuple:\n",
    "    \"\"\"\n",
    "    Initialize the VetBERT model and tokenizer.\n",
    "    \n",
    "    Args:\n",
    "        model_name (str): Name of the pretrained model\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (tokenizer, model, device)\n",
    "        \n",
    "    Raises:\n",
    "        RuntimeError: If model initialization fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize tokenizer and model\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModel.from_pretrained(model_name)\n",
    "        \n",
    "        # Set up device\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = model.to(device)\n",
    "        \n",
    "        logger.info(f\"Model initialized successfully on {device}\")\n",
    "        return tokenizer, model, device\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to initialize model: {str(e)}\")\n",
    "        raise RuntimeError(f\"Model initialization failed: {str(e)}\")\n",
    "\n",
    "# Initialize model and tokenizer\n",
    "try:\n",
    "    tokenizer, model, device = initialize_model()\n",
    "except Exception as e:\n",
    "    logger.error(f\"Model setup failed: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb3d62df-7691-4847-a116-acbf8e7665d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vetbert_embeddings(\n",
    "    texts: List[str],\n",
    "    tokenizer: AutoTokenizer,\n",
    "    model: AutoModel,\n",
    "    device: torch.device,\n",
    "    max_length: int = 512\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate VetBERT embeddings for a list of texts.\n",
    "    \n",
    "    Args:\n",
    "        texts (List[str]): List of input texts\n",
    "        tokenizer: VetBERT tokenizer\n",
    "        model: VetBERT model\n",
    "        device: Torch device\n",
    "        max_length (int): Maximum sequence length\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: Text embeddings\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If input texts are invalid\n",
    "        RuntimeError: If embedding generation fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not texts:\n",
    "            raise ValueError(\"Empty text list provided\")\n",
    "            \n",
    "        # Tokenize the input texts\n",
    "        inputs = tokenizer(\n",
    "            texts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length\n",
    "        )\n",
    "        \n",
    "        # Move inputs to device\n",
    "        inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "        \n",
    "        # Generate embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "        # Use CLS token embedding\n",
    "        cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "        embeddings = cls_embedding.cpu().numpy()\n",
    "        \n",
    "        logger.info(f\"Generated embeddings for {len(texts)} texts\")\n",
    "        return embeddings\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to generate embeddings: {str(e)}\")\n",
    "        raise RuntimeError(f\"Embedding generation failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "691224f2-bde0-4012-a347-60dfcc8e670b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fhoss\\AppData\\Local\\Temp\\ipykernel_6300\\1513286368.py:26: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  data = data.applymap(lambda x: x.lower() if isinstance(x, str) else x)\n",
      "2025-09-12 17:04:33,639 - INFO - Generated embeddings for 1000 texts\n",
      "2025-09-12 17:04:33,639 - INFO - Generated embeddings for 1000 texts\n",
      "C:\\Users\\fhoss\\AppData\\Local\\Temp\\ipykernel_6300\\1513286368.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_clinic['vetbert_vector'] = list(embeddings)\n",
      "2025-09-12 17:04:33,970 - INFO - Processed 1000 clinical notes\n",
      "C:\\Users\\fhoss\\AppData\\Local\\Temp\\ipykernel_6300\\1513286368.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_clinic['vetbert_vector'] = list(embeddings)\n",
      "2025-09-12 17:04:33,970 - INFO - Processed 1000 clinical notes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (1000, 4)\n"
     ]
    }
   ],
   "source": [
    "def load_and_process_data(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load and preprocess the dataset.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the CSV file\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Processed clinical notes data with embeddings\n",
    "        \n",
    "    Raises:\n",
    "        FileNotFoundError: If data file doesn't exist\n",
    "        ValueError: If data processing fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check file existence\n",
    "        if not os.path.exists(file_path):\n",
    "            raise FileNotFoundError(f\"Data file not found: {file_path}\")\n",
    "            \n",
    "        # Load data\n",
    "        data = pd.read_csv(file_path)\n",
    "        if data.empty:\n",
    "            raise ValueError(\"Empty dataset\")\n",
    "            \n",
    "        # Preprocess data\n",
    "        data = data.applymap(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "        data_clinic = data[data['record_type'] == 'clinical notes']\n",
    "        \n",
    "        if data_clinic.empty:\n",
    "            raise ValueError(\"No clinical notes found in dataset\")\n",
    "            \n",
    "        # Generate embeddings\n",
    "        texts_clinic = data_clinic['text'].tolist()\n",
    "        embeddings = get_vetbert_embeddings(texts_clinic, tokenizer, model, device)\n",
    "        \n",
    "        # Add embeddings to DataFrame\n",
    "        data_clinic['vetbert_vector'] = list(embeddings)\n",
    "        \n",
    "        logger.info(f\"Processed {len(data_clinic)} clinical notes\")\n",
    "        return data_clinic\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Data processing failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Load and process data\n",
    "try:\n",
    "    data_clinic = load_and_process_data('Raw_Data/pet-health-symptoms-dataset.csv')\n",
    "    print(f\"Dataset shape: {data_clinic.shape}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to load and process data: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8e83337-12b4-4627-9f78-6b5fb2bcdf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clinic['Dimension'] = data_clinic['vetbert_vector'].map(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea7ce826-1edc-4249-8764-0ebef3fc5a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n"
     ]
    }
   ],
   "source": [
    "# Extract the vector from the Series\n",
    "vector = data_clinic.iloc[0]['vetbert_vector']\n",
    "\n",
    "# Print the vector\n",
    "print(len(vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50167005-cef4-44e1-972e-75ab6b083d9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clinic.groupby('Dimension').size().reset_index().iloc[0]['Dimension']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a322d0e-3ac4-40ea-ae08-46d5a8034fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the data with the embeding vector\n",
    "data_clinic.to_pickle('data_Clinic.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d0c4f4-d3f5-4317-a1cd-f0a500c91050",
   "metadata": {},
   "source": [
    "# Vector Data base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c773c2d-02c4-4fdc-9fa0-d9860415040b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the vector csv\n",
    "data_clinic=pd.read_pickle('data_Clinic.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69d5c9c4-656a-4f3f-81c1-4fcd399fe234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding special index to each row in a databse\n",
    "converted_df = [\n",
    "    {\n",
    "        \"id\": idx + 1,\n",
    "        \"vector\":row[\"vetbert_vector\"],\n",
    "        \"text\": row[\"text\"],\n",
    "        \"condition\": row[\"condition\"]\n",
    "    }\n",
    "    for idx, row in data_clinic.iterrows()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7cb97a2a-bd88-4c7d-9d8e-5d85c5a94e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-12 17:07:57,374 - INFO - HTTP Request: GET http://localhost:6333 \"HTTP/1.1 200 OK\"\n",
      "C:\\Users\\fhoss\\AppData\\Local\\Temp\\ipykernel_6300\\3991484778.py:35: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  client.recreate_collection(\n",
      "2025-09-12 17:07:57,841 - INFO - HTTP Request: DELETE http://localhost:6333/collections/vet_notes \"HTTP/1.1 200 OK\"\n",
      "2025-09-12 17:07:58,699 - INFO - HTTP Request: PUT http://localhost:6333/collections/vet_notes \"HTTP/1.1 200 OK\"\n",
      "2025-09-12 17:07:58,701 - INFO - Initialized Qdrant collection 'vet_notes' with vector size 768\n"
     ]
    }
   ],
   "source": [
    "def initialize_qdrant(\n",
    "    host: str = \"localhost\",\n",
    "    port: int = 6333,\n",
    "    collection_name: str = \"vet_notes\",\n",
    "    vector_size: int = None\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Initialize Qdrant vector database.\n",
    "    \n",
    "    Args:\n",
    "        host (str): Qdrant server host\n",
    "        port (int): Qdrant server port\n",
    "        collection_name (str): Name of the collection\n",
    "        vector_size (int): Size of vectors\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (QdrantClient, collection_name)\n",
    "        \n",
    "    Raises:\n",
    "        ConnectionError: If connection to Qdrant fails\n",
    "        ValueError: If vector size is invalid\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Connect to Qdrant\n",
    "        client = QdrantClient(url=f\"http://{host}:{port}\")\n",
    "        \n",
    "        # Validate vector size\n",
    "        if not vector_size:\n",
    "            vector_size = data_clinic.groupby('Dimension').size().reset_index().iloc[0]['Dimension']\n",
    "        \n",
    "        if vector_size <= 0:\n",
    "            raise ValueError(\"Invalid vector size\")\n",
    "            \n",
    "        # Create or recreate collection\n",
    "        client.recreate_collection(\n",
    "            collection_name=collection_name,\n",
    "            vectors_config=models.VectorParams(\n",
    "                size=vector_size,\n",
    "                distance=models.Distance.COSINE\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Initialized Qdrant collection '{collection_name}' with vector size {vector_size}\")\n",
    "        return client, collection_name\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to initialize Qdrant: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Initialize Qdrant\n",
    "try:\n",
    "    client, collection_name = initialize_qdrant()\n",
    "except Exception as e:\n",
    "    logger.error(f\"Qdrant initialization failed: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "514c4366-6a61-47af-ba72-9f7420d49be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-12 17:08:01,995 - INFO - Prepared 1000 points for insertion\n",
      "2025-09-12 17:08:03,969 - INFO - HTTP Request: PUT http://localhost:6333/collections/vet_notes/points?wait=true \"HTTP/1.1 200 OK\"\n",
      "2025-09-12 17:08:03,982 - INFO - Successfully inserted 1000 vectors\n",
      "2025-09-12 17:08:04,039 - INFO - HTTP Request: GET http://localhost:6333/collections/vet_notes \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of points in collection: 1000\n"
     ]
    }
   ],
   "source": [
    "def prepare_points(data: List[Dict]) -> List[PointStruct]:\n",
    "    \"\"\"\n",
    "    Prepare data points for Qdrant vector insertion.\n",
    "    \n",
    "    Args:\n",
    "        data: List of dictionaries with keys 'id', 'vector', 'text', 'condition'\n",
    "        \n",
    "    Returns:\n",
    "        List[PointStruct]: Prepared points for Qdrant\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If input data is invalid\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not data:\n",
    "            raise ValueError(\"Empty data provided\")\n",
    "            \n",
    "        points = []\n",
    "        for record in data:\n",
    "            # Validate required fields\n",
    "            required_fields = ['id', 'vector', 'text', 'condition']\n",
    "            if not all(field in record for field in required_fields):\n",
    "                raise ValueError(f\"Missing required fields. Need {required_fields}\")\n",
    "                \n",
    "            point = PointStruct(\n",
    "                id=record['id'],\n",
    "                vector=record['vector'],\n",
    "                payload={\n",
    "                    \"text\": record['text'],\n",
    "                    \"condition\": record['condition']\n",
    "                }\n",
    "            )\n",
    "            points.append(point)\n",
    "            \n",
    "        logger.info(f\"Prepared {len(points)} points for insertion\")\n",
    "        return points\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to prepare points: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def insert_vectors(\n",
    "    client: QdrantClient,\n",
    "    collection_name: str,\n",
    "    data: pd.DataFrame\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Insert vectors into Qdrant collection.\n",
    "    \n",
    "    Args:\n",
    "        client: Qdrant client\n",
    "        collection_name: Name of the collection\n",
    "        data: DataFrame with vector data\n",
    "        \n",
    "    Raises:\n",
    "        RuntimeError: If vector insertion fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert DataFrame to list of dictionaries\n",
    "        converted_df = [\n",
    "            {\n",
    "                \"id\": idx + 1,\n",
    "                \"vector\": row[\"vetbert_vector\"],\n",
    "                \"text\": row[\"text\"],\n",
    "                \"condition\": row[\"condition\"]\n",
    "            }\n",
    "            for idx, row in data.iterrows()\n",
    "        ]\n",
    "        \n",
    "        # Prepare and insert points\n",
    "        points = prepare_points(converted_df)\n",
    "        client.upsert(collection_name=collection_name, points=points)\n",
    "        \n",
    "        logger.info(f\"Successfully inserted {len(points)} vectors\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to insert vectors: {str(e)}\")\n",
    "        raise RuntimeError(f\"Vector insertion failed: {str(e)}\")\n",
    "\n",
    "# Insert vectors into Qdrant\n",
    "try:\n",
    "    insert_vectors(client, collection_name, data_clinic)\n",
    "    \n",
    "    # Verify insertion\n",
    "    collection_info = client.get_collection(collection_name=collection_name)\n",
    "    print(f\"Number of points in collection: {collection_info.points_count}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Vector database population failed: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c2dea00a-5510-4472-8dfc-6076316ebe22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-12 17:09:34,997 - INFO - HTTP Request: GET http://localhost:6333/collections/vet_notes \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of points in the collection: 1000\n"
     ]
    }
   ],
   "source": [
    "#Check the vectorDatabase created\n",
    "\n",
    "collection_name = \"vet_notes\"\n",
    "\n",
    "collection_info = client.get_collection(collection_name=collection_name)\n",
    "print(f\"Number of points in the collection: {collection_info.points_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b96099-c084-403a-b49a-480281abd0c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
