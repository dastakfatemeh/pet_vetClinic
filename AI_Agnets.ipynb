{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7072ab2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from qdrant_client import QdrantClient\n",
    "from transformers import pipeline\n",
    "from typing import Optional, Union, List, Tuple\n",
    "import torch.nn.functional as F\n",
    "import logging\n",
    "from qdrant_client import QdrantClient, models\n",
    "from qdrant_client.http.models import VectorParams, Distance, PointStruct, Filter, FieldCondition, MatchValue\n",
    "from huggingface_hub import login\n",
    "from HF_t import hf_token_read\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification,T5Tokenizer,T5ForConditionalGeneration\n",
    "import sentencepiece as spm\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "\n",
    "\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ad6ed1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device if not provided\n",
    "device = None\n",
    "if device is None:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf04548",
   "metadata": {},
   "source": [
    "# Connect to the Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c85d2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://localhost:6333 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:6333/collections/vet_notes \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "# Initilizae the Qdrant\n",
    "from qdrant_client import QdrantClient, models\n",
    "from qdrant_client.http.models import VectorParams, Distance, PointStruct, Filter, FieldCondition, MatchValue\n",
    "\n",
    "#initialize with a local path to persist data on disk without a server:\n",
    "#client = QdrantClient('./PetHealth_Chatbot/Vector_Database')\n",
    "# Connect to a local Qdrant instance\n",
    "client = QdrantClient(url=\"http://localhost:6333\") # Or specify your actual local host/port\n",
    "\n",
    "collection_name = \"vet_notes\"\n",
    "\n",
    "collection_info = client.get_collection(collection_name=collection_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d47628",
   "metadata": {},
   "source": [
    "# Loading Models\n",
    "First, we'll load two different models:\n",
    "1. VetBERT model for generating embeddings (vector representations)\n",
    "2. Classification model for predicting conditions\n",
    "3. Generating tex model for Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7f6ec476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading VetBERT model for embeddings...\n",
      "VetBERT model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# 1. Load VetBERT model for embeddings\n",
    "print(\"Loading VetBERT model for embeddings...\")\n",
    "vetbert_model = AutoModel.from_pretrained(\"havocy28/VetBERT\")\n",
    "vetbert_tokenizer = AutoTokenizer.from_pretrained(\"havocy28/VetBERT\")\n",
    "\n",
    "# Check if a GPU is available and move the model to the GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# vetbert_model.to(device)\n",
    "print(\"VetBERT model loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7bc0e114",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. loading the model from hugging face that trained on owner notes for classification\n",
    "#Use the token for reading to load the model in huging face\n",
    "hf_token_write = \"hf_token_read\"\n",
    "\n",
    "# In your notebook\n",
    "# from huggingface_hub import login\n",
    "# from HF_t import hf_token_read  # Import your token securely\n",
    "\n",
    "login(token=hf_token_read)\n",
    "\n",
    "# Replace \"your-username/my-awesome-fine-tuned-model\" with your actual repository ID\n",
    "repo_id = \"fdastak/model_calssification\" \n",
    "\n",
    "# Load the fine-tuned model and its tokenizer\n",
    "model_Class= AutoModelForSequenceClassification.from_pretrained(repo_id)\n",
    "tokenizer_class = AutoTokenizer.from_pretrained(repo_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c6a9c926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ClinicalT5-base model for summarization...\n"
     ]
    }
   ],
   "source": [
    "#3. Loading the model for summarization...\n",
    "print(\"Loading ClinicalT5-base model for summarization...\")\n",
    "# Load tokenizer and model from Hugging Face hub\n",
    "# Load small T5 model (t5-small or t5-base)\n",
    "model_name = \"t5-small\"\n",
    "\n",
    "tokenizer_textg = T5Tokenizer.from_pretrained(model_name)\n",
    "model_textg = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ad07db",
   "metadata": {},
   "source": [
    "# Create an embeding class that can be used by differnt AI agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "af7771a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mixin with the shared embedding method\n",
    "class VetBERTMixin:\n",
    "    def get_vetbert_embeddings(\n",
    "        self, \n",
    "        user_input: str, \n",
    "        return_numpy: bool = True\n",
    "    ) -> Union[torch.Tensor, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Generate embeddings using VetBERT model.\n",
    "\n",
    "        Args:\n",
    "            user_input (str): Text to embed\n",
    "            return_numpy (bool): Return numpy array if True, else torch tensor\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor or np.ndarray: Embedding vector\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        inputs = self.tokenizer(\n",
    "            user_input,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        )\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            embeddings = outputs.last_hidden_state[:, 0, :]  # CLS token\n",
    "\n",
    "        if return_numpy:\n",
    "            embeddings = embeddings.cpu().numpy()\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb5ec88",
   "metadata": {},
   "source": [
    "# Agent System\n",
    "\n",
    "We'll create two agents that work together:\n",
    "1. **Classification Agent**: Identifies the pet's condition from user input\n",
    "2. **Retrieval Agent**: Finds relevant veterinary notes based on the identified condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0d435c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationAgent:\n",
    "    def __init__(self, model, tokenizer, device):\n",
    "        self.device = device\n",
    "        self.model = model.to(self.device) # Move model to device\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label_to_condition = {\n",
    "            0: \"digestive issues\",\n",
    "            1: \"ear infections\",\n",
    "            2: \"mobility problems\",\n",
    "            3: \"parasites\",\n",
    "            4: \"skin irritations\"\n",
    "        }\n",
    "\n",
    "    def predict_condition(self,user_input) -> tuple:\n",
    "        \"\"\"\n",
    "        Predict the condition for a given input text using the classification model.\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (predicted_label, confidence_score)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Set to eval mode\n",
    "            self.model.eval()\n",
    "            \n",
    "            # Tokenize input for classifier\n",
    "            inputs = self.tokenizer(\n",
    "                user_input,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=512\n",
    "            )\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "            \n",
    "            # Get predictions\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "                logits = outputs.logits\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                confidence_score, pred_label = torch.max(probs, dim=1)\n",
    "                \n",
    "                logger.info(f\"Successfully generated prediction with confidence {confidence_score.item():.4f}\")\n",
    "                return pred_label.item(), confidence_score.item()\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Prediction failed: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def identify_condition(self, user_input: str) -> tuple:\n",
    "        \"\"\"\n",
    "        Identify pet's condition from user input.\n",
    "            \n",
    "        Args:\n",
    "        user_input (str): Description of pet's symptoms\n",
    "                \n",
    "        Returns:\n",
    "        tuple: (condition_name, confidence_score)\n",
    "            \"\"\"\n",
    "        try:\n",
    "            # Get prediction\n",
    "            predicted_label, confidence = self.predict_condition(user_input)\n",
    "                \n",
    "            # Convert to condition name\n",
    "            condition_name = self.label_to_condition.get(predicted_label, \"unknown condition\")\n",
    "                \n",
    "            logger.info(f\"Identified condition: {condition_name} with confidence: {confidence:.4f}\")\n",
    "            return condition_name, confidence\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Error in classification: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "class RetrievalAgent(VetBERTMixin):\n",
    "    def __init__(self, model, tokenizer, device, qdrant_client, collection_name):\n",
    "        self.device = device\n",
    "        self.model = model.to(self.device) # Move model to device\n",
    "        self.tokenizer = tokenizer\n",
    "        self.client = qdrant_client\n",
    "        self.collection_name = collection_name\n",
    "\n",
    "    def find_similar_cases(self, user_input: str, condition: str, limit: int = 3) -> list:\n",
    "        \"\"\"\n",
    "        Find similar veterinary cases based on input and condition.\n",
    "        \n",
    "        Args:\n",
    "            user_input (str): User's description of symptoms\n",
    "            condition (str): Identified condition to filter by\n",
    "            limit (int): Number of similar cases to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            list: Similar cases with their scores\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            # Generate embeddings for the query\n",
    "            query_vector = self.get_vetbert_embeddings(\n",
    "                user_input,\n",
    "                return_numpy=True\n",
    "            )\n",
    "            \n",
    "            # Set up condition filter\n",
    "            condition_filter = Filter(\n",
    "                must=[FieldCondition(key=\"condition\", match=MatchValue(value=condition))]\n",
    "            )\n",
    "            \n",
    "            # Search for similar cases\n",
    "            results = self.client.search(\n",
    "                collection_name=self.collection_name,\n",
    "                query_vector=query_vector[0],\n",
    "                limit=limit,\n",
    "                query_filter=condition_filter,\n",
    "                with_payload=True\n",
    "            )\n",
    "            # qp = client.query_points(\n",
    "            # collection_name=collection_name,\n",
    "            # query=query_vector[0].tolist(),\n",
    "            # limit=3,\n",
    "            # query_filter=filter_by_category,\n",
    "            # with_payload=True,\n",
    "            #  )\n",
    "            # results = qp.points\n",
    "\n",
    "            \n",
    "            logger.info(f\"Found {len(results)} similar cases for condition: {condition}\")\n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Error in retrieval: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "class communicationAgent():\n",
    "    def __init__(self,model,tokenizer,device):\n",
    "        self.device = device\n",
    "        self.model = model.to(self.device) # Move model to device\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def output_vet(self,text1,text2,text3):\n",
    "        # Prepare the input text for T5\n",
    "        list=[text1,text2,text3]\n",
    "        prompt = \"summarize: \" + \" \".join(list)\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        outputs = self.model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=150,\n",
    "            num_beams=4,\n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "        summary = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        # Carefully craft a conversational question\n",
    "        appointment_question = (\n",
    "            \"\\n\\nWould you like to schedule an appointment with our veterinary team \"\n",
    "            \"to address these health findings and discuss the most effective treatment options for your pet?\"\n",
    "        )\n",
    "\n",
    "        # Combine summary and question\n",
    "        final_output = final_output = (\n",
    "            f\"Here are my findings: {summary.strip()}\\n\"\n",
    "            f\"{appointment_question}\"\n",
    "        )\n",
    "        print(final_output)\n",
    "        return final_output\n",
    "        #Would you like to schedule an appointment with our veterinary team to provide these treatments and ensure your pet’s comfort?\n",
    "# Initialize agents\n",
    "classification_agent = ClassificationAgent(\n",
    "    model=model_Class,\n",
    "    tokenizer=tokenizer_class,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "retrieval_agent = RetrievalAgent(\n",
    "    model=vetbert_model,\n",
    "    tokenizer=vetbert_tokenizer,\n",
    "    device=device,\n",
    "    qdrant_client=client,\n",
    "    collection_name=collection_name\n",
    ")\n",
    "\n",
    "communicationAgent = communicationAgent(\n",
    "    model=model_textg,\n",
    "    tokenizer=tokenizer_textg,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bae4b261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing query: Something grows on one of my dog ears\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Successfully generated prediction with confidence 0.5530\n",
      "INFO:__main__:Identified condition: ear infections with confidence: 0.5530\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Results:\n",
      "Identified Condition: ear infections\n",
      "Confidence Score: 0.5530\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fhoss\\AppData\\Local\\Temp\\ipykernel_23236\\866328814.py:107: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  results = self.client.search(\n",
      "INFO:httpx:HTTP Request: POST http://localhost:6333/collections/vet_notes/points/search \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Found 3 similar cases for condition: ear infections\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar Cases for ear infections:\n",
      "Score: 0.8421\n",
      "Case: evaluate conformation of ear canal (stenotic breeds).\n",
      "Condition: ear infections\n",
      "\n",
      "Score: 0.8395\n",
      "Case: use tris-edta containing ear flush for pseudomonas infections.\n",
      "Condition: ear infections\n",
      "\n",
      "Score: 0.8367\n",
      "Case: use ear wick or packing with medication for severe stenosis.\n",
      "Condition: ear infections\n",
      "\n",
      "Here are my findings: tris-edta contains ear flush for pseudomonas infections. use ear wick or packing with medication for severe stenosis.\n",
      "\n",
      "\n",
      "Would you like to schedule an appointment with our veterinary team to address these health findings and discuss the most effective treatment options for your pet?\n",
      "Summary of Top Cases:\n",
      "Here are my findings: tris-edta contains ear flush for pseudomonas infections. use ear wick or packing with medication for severe stenosis.\n",
      "\n",
      "\n",
      "Would you like to schedule an appointment with our veterinary team to address these health findings and discuss the most effective treatment options for your pet?\n",
      "\n",
      "Processing query: My cat has been scratching a lot and has some red spots\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Successfully generated prediction with confidence 0.8247\n",
      "INFO:__main__:Identified condition: parasites with confidence: 0.8247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Results:\n",
      "Identified Condition: parasites\n",
      "Confidence Score: 0.8247\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:6333/collections/vet_notes/points/search \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Found 3 similar cases for condition: parasites\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar Cases for parasites:\n",
      "Score: 0.8267\n",
      "Case: ear mites in canal; clean and apply selamectin.\n",
      "Condition: parasites\n",
      "\n",
      "Score: 0.8093\n",
      "Case: sarcoptic mange; prescribe oral ivermectin.\n",
      "Condition: parasites\n",
      "\n",
      "Score: 0.8061\n",
      "Case: flea dirt present on coat combing; ctenocephalides felis infestation confirmed.\n",
      "Condition: parasites\n",
      "\n",
      "Here are my findings: ear mites in canal; clean and apply selamectin. ctenocephalides felis infestation confirmed.\n",
      "\n",
      "\n",
      "Would you like to schedule an appointment with our veterinary team to address these health findings and discuss the most effective treatment options for your pet?\n",
      "Summary of Top Cases:\n",
      "Here are my findings: ear mites in canal; clean and apply selamectin. ctenocephalides felis infestation confirmed.\n",
      "\n",
      "\n",
      "Would you like to schedule an appointment with our veterinary team to address these health findings and discuss the most effective treatment options for your pet?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage with the agent system\n",
    "def process_user_query(user_input: str):\n",
    "    \"\"\"Process user query through both agents\"\"\"\n",
    "    print(f\"Processing query: {user_input}\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Use Classification Agent to identify the condition\n",
    "        condition, confidence = classification_agent.identify_condition(user_input)\n",
    "        print(f\"Classification Results:\")\n",
    "        print(f\"Identified Condition: {condition}\")\n",
    "        print(f\"Confidence Score: {confidence:.4f}\\n\")\n",
    "        \n",
    "        # Step 2: Use Retrieval Agent to find similar cases\n",
    "        similar_cases = retrieval_agent.find_similar_cases(user_input, condition)\n",
    "        \n",
    "        print(f\"Similar Cases for {condition}:\")\n",
    "        for case in similar_cases:\n",
    "            print(f\"Score: {case.score:.4f}\")\n",
    "            print(f\"Case: {case.payload['text']}\")\n",
    "            print(f\"Condition: {case.payload['condition']}\\n\")\n",
    "\n",
    "       # step 3: Use communicationAgent to summarize the top 3 cases\n",
    "        if len(similar_cases) == 3:\n",
    "            summary = communicationAgent.output_vet(\n",
    "                similar_cases[0].payload['text'],\n",
    "                similar_cases[1].payload['text'],\n",
    "                similar_cases[2].payload['text']\n",
    "            )\n",
    "        elif len(similar_cases) == 2:\n",
    "            summary = communicationAgent.output_vet(\n",
    "                similar_cases[0].payload['text'],\n",
    "                similar_cases[1].payload['text'],\n",
    "            )\n",
    "        elif len(similar_cases) == 1:\n",
    "            summary = communicationAgent.output_vet(\n",
    "                similar_cases[0].payload['text'])\n",
    "        else:\n",
    "            print(\"Not enough similar cases to summarize.\\n\")\n",
    "        print(f\"Summary of Top Cases:\\n{summary}\\n\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing query: {str(e)}\")\n",
    "\n",
    "# Test the system\n",
    "user_input = \"Something grows on one of my dog ears\"\n",
    "process_user_query(user_input)\n",
    "\n",
    "# Try another example\n",
    "user_input2 = \"My cat has been scratching a lot and has some red spots\"\n",
    "process_user_query(user_input2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fa9109",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
