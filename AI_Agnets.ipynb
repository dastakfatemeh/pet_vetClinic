{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7072ab2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from qdrant_client import QdrantClient\n",
    "from transformers import pipeline\n",
    "import huggingface\n",
    "from typing import Optional, Union, List, Tuple\n",
    "import torch.nn.functional as F\n",
    "import logging\n",
    "from qdrant_client import QdrantClient, models\n",
    "from qdrant_client.http.models import VectorParams, Distance, PointStruct, Filter, FieldCondition, MatchValue\n",
    "from huggingface_hub import login\n",
    "from HF_t import hf_token_read\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf04548",
   "metadata": {},
   "source": [
    "# Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c85d2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://localhost:6333 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:6333/collections/vet_notes \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:6333/collections/vet_notes \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "# Initilizae the Qdrant\n",
    "from qdrant_client import QdrantClient, models\n",
    "from qdrant_client.http.models import VectorParams, Distance, PointStruct, Filter, FieldCondition, MatchValue\n",
    "\n",
    "#initialize with a local path to persist data on disk without a server:\n",
    "#client = QdrantClient('./PetHealth_Chatbot/Vector_Database')\n",
    "# Connect to a local Qdrant instance\n",
    "client = QdrantClient(url=\"http://localhost:6333\") # Or specify your actual local host/port\n",
    "\n",
    "collection_name = \"vet_notes\"\n",
    "\n",
    "collection_info = client.get_collection(collection_name=collection_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff11d35",
   "metadata": {},
   "source": [
    "# Loading Models\n",
    "\n",
    "First, we'll load two different models:\n",
    "1. VetBERT model for generating embeddings (vector representations)\n",
    "2. Classification model for predicting conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7f6ec476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading VetBERT model for embeddings...\n",
      "VetBERT model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Load VetBERT model for embeddings\n",
    "print(\"Loading VetBERT model for embeddings...\")\n",
    "vetbert_model = AutoModel.from_pretrained(\"havocy28/VetBERT\")\n",
    "vetbert_tokenizer = AutoTokenizer.from_pretrained(\"havocy28/VetBERT\")\n",
    "\n",
    "# Check if a GPU is available and move the model to the GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vetbert_model.to(device)\n",
    "print(\"VetBERT model loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "026ec1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vetbert_embeddings(\n",
    "    text: str,\n",
    "    model: AutoModel,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    device: torch.device,\n",
    "    return_numpy: bool = True\n",
    ") -> Union[torch.Tensor, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Generate embeddings using VetBERT model.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to generate embeddings for\n",
    "        model: VetBERT model\n",
    "        tokenizer: VetBERT tokenizer\n",
    "        device: Computation device (CPU/GPU)\n",
    "        return_numpy: If True, returns numpy array (for Qdrant), if False returns PyTorch tensor\n",
    "        \n",
    "    Returns:\n",
    "        Union[torch.Tensor, np.ndarray]: Text embeddings from the [CLS] token\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ensure model is in eval mode\n",
    "        model.eval()\n",
    "        \n",
    "        # Tokenize the input\n",
    "        inputs = tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        )\n",
    "        \n",
    "        # Move inputs to device\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Generate embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            # Get the [CLS] token embedding (first token)\n",
    "            embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "            \n",
    "        # Convert to numpy if requested\n",
    "        if return_numpy:\n",
    "            embeddings = embeddings.cpu().numpy()\n",
    "            \n",
    "        return embeddings\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error generating embeddings: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a85a6e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the model from hugging face\n",
    "#Use the token for reading to load the model in huging face\n",
    "hf_token_write = \"hf_token_read\"\n",
    "\n",
    "# In your notebook\n",
    "# from huggingface_hub import login\n",
    "# from HF_t import hf_token_read  # Import your token securely\n",
    "\n",
    "login(token=hf_token_read)\n",
    "\n",
    "# Replace \"your-username/my-awesome-fine-tuned-model\" with your actual repository ID\n",
    "repo_id = \"fdastak/model_calssification\" \n",
    "\n",
    "# Load the fine-tuned model and its tokenizer\n",
    "model_Class= AutoModelForSequenceClassification.from_pretrained(repo_id)\n",
    "tokenizer_class = AutoTokenizer.from_pretrained(repo_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "34372567",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Successfully generated prediction with confidence 0.5530\n",
      "C:\\Users\\fhoss\\AppData\\Local\\Temp\\ipykernel_24240\\267898521.py:57: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  results = client.search(\n",
      "INFO:httpx:HTTP Request: POST http://localhost:6333/collections/vet_notes/points/search \"HTTP/1.1 200 OK\"\n",
      "C:\\Users\\fhoss\\AppData\\Local\\Temp\\ipykernel_24240\\267898521.py:57: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  results = client.search(\n",
      "INFO:httpx:HTTP Request: POST http://localhost:6333/collections/vet_notes/points/search \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings shape: (1, 768)\n",
      "\n",
      "Classification results:\n",
      "Predicted Label: 1 (ear infections)\n",
      "Confidence Score: 0.5530\n",
      "\n",
      "Similar cases from the database:\n",
      "Score: 0.8421, Text: evaluate conformation of ear canal (stenotic breeds)., Category: ear infections\n",
      "Score: 0.8395, Text: use tris-edta containing ear flush for pseudomonas infections., Category: ear infections\n",
      "Score: 0.8367, Text: use ear wick or packing with medication for severe stenosis., Category: ear infections\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "input_text = \"Something grows on one of my dog ears\"  # Your input text\n",
    "\n",
    "# Create a mapping dictionary for label to condition\n",
    "label_to_condition = {\n",
    "    0: \"digestive issues\",\n",
    "    1: \"ear infections\",\n",
    "    2: \"mobility problems\",\n",
    "    3: \"parasites\",\n",
    "    4: \"skin irritations\"\n",
    "}\n",
    "\n",
    "def get_condition_name(label: int) -> str:\n",
    "    \"\"\"\n",
    "    Convert numeric label to condition name.\n",
    "    \n",
    "    Args:\n",
    "        label (int): Numeric label from the classifier\n",
    "        \n",
    "    Returns:\n",
    "        str: Corresponding condition name\n",
    "    \"\"\"\n",
    "    return label_to_condition.get(label, \"unknown condition\")\n",
    "\n",
    "try:\n",
    "    # Step 1: Generate embeddings using VetBERT for vector search (in numpy format for Qdrant)\n",
    "    query_vector = get_vetbert_embeddings(\n",
    "        text=input_text,\n",
    "        model=vetbert_model,\n",
    "        tokenizer=vetbert_tokenizer,\n",
    "        device=device,\n",
    "        return_numpy=True  # Get numpy array directly for Qdrant\n",
    "    )\n",
    "    print(f\"Generated embeddings shape: {query_vector.shape}\")\n",
    "    \n",
    "    # Step 2: Get condition prediction using classification model\n",
    "    predicted_label, confidence = predict_condition(\n",
    "        text=input_text,\n",
    "        classifier_model=model_Class,  # Use classification model for predictions\n",
    "        classifier_tokenizer=tokenizer_class,  # Use classification tokenizer\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Convert numeric label to condition name\n",
    "    condition_name = get_condition_name(predicted_label)\n",
    "    \n",
    "    print(f\"\\nClassification results:\")\n",
    "    print(f\"Predicted Label: {predicted_label} ({condition_name})\")\n",
    "    print(f\"Confidence Score: {confidence:.4f}\")\n",
    "    \n",
    "    # Step 3: Search similar cases in Qdrant\n",
    "    filter_by_category = Filter(\n",
    "        must=[FieldCondition(key=\"condition\", match=MatchValue(value=condition_name))]\n",
    "    )\n",
    "    \n",
    "    # Using search instead of query_points since it's working\n",
    "    results = client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=query_vector[0],\n",
    "        limit=3,\n",
    "        query_filter=filter_by_category,\n",
    "        with_payload=True  # returns metadata like text and category\n",
    "    )\n",
    "    \n",
    "    print(\"\\nSimilar cases from the database:\")\n",
    "    for result in results:\n",
    "        print(f\"Score: {result.score:.4f}, Text: {result.payload['text']}, Category: {result.payload['condition']}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error during processing: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb5ec88",
   "metadata": {},
   "source": [
    "# Agent System\n",
    "\n",
    "We'll create two agents that work together:\n",
    "1. **Classification Agent**: Identifies the pet's condition from user input\n",
    "2. **Retrieval Agent**: Finds relevant veterinary notes based on the identified condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0d435c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationAgent:\n",
    "    def __init__(self, model, tokenizer, device):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.label_to_condition = {\n",
    "            0: \"digestive issues\",\n",
    "            1: \"ear infections\",\n",
    "            2: \"mobility problems\",\n",
    "            3: \"parasites\",\n",
    "            4: \"skin irritations\"\n",
    "        }\n",
    "    \n",
    "    def identify_condition(self, user_input: str) -> tuple:\n",
    "        \"\"\"\n",
    "        Identify pet's condition from user input.\n",
    "        \n",
    "        Args:\n",
    "            user_input (str): Description of pet's symptoms\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (condition_name, confidence_score)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Get prediction\n",
    "            predicted_label, confidence = predict_condition(\n",
    "                text=user_input,\n",
    "                classifier_model=self.model,\n",
    "                classifier_tokenizer=self.tokenizer,\n",
    "                device=self.device\n",
    "            )\n",
    "            \n",
    "            # Convert to condition name\n",
    "            condition_name = self.label_to_condition.get(predicted_label, \"unknown condition\")\n",
    "            \n",
    "            logger.info(f\"Identified condition: {condition_name} with confidence: {confidence:.4f}\")\n",
    "            return condition_name, confidence\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in classification: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "class RetrievalAgent:\n",
    "    def __init__(self, model, tokenizer, device, qdrant_client, collection_name):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.client = qdrant_client\n",
    "        self.collection_name = collection_name\n",
    "    \n",
    "    def find_similar_cases(self, user_input: str, condition: str, limit: int = 3) -> list:\n",
    "        \"\"\"\n",
    "        Find similar veterinary cases based on input and condition.\n",
    "        \n",
    "        Args:\n",
    "            user_input (str): User's description of symptoms\n",
    "            condition (str): Identified condition to filter by\n",
    "            limit (int): Number of similar cases to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            list: Similar cases with their scores\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Generate embeddings for the query\n",
    "            query_vector = get_vetbert_embeddings(\n",
    "                text=user_input,\n",
    "                model=self.model,\n",
    "                tokenizer=self.tokenizer,\n",
    "                device=self.device,\n",
    "                return_numpy=True\n",
    "            )\n",
    "            \n",
    "            # Set up condition filter\n",
    "            condition_filter = Filter(\n",
    "                must=[FieldCondition(key=\"condition\", match=MatchValue(value=condition))]\n",
    "            )\n",
    "            \n",
    "            # Search for similar cases\n",
    "            results = self.client.search(\n",
    "                collection_name=self.collection_name,\n",
    "                query_vector=query_vector[0],\n",
    "                limit=limit,\n",
    "                query_filter=condition_filter,\n",
    "                with_payload=True\n",
    "            )\n",
    "            \n",
    "            logger.info(f\"Found {len(results)} similar cases for condition: {condition}\")\n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in retrieval: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "# Initialize agents\n",
    "classification_agent = ClassificationAgent(\n",
    "    model=model_Class,\n",
    "    tokenizer=tokenizer_class,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "retrieval_agent = RetrievalAgent(\n",
    "    model=vetbert_model,\n",
    "    tokenizer=vetbert_tokenizer,\n",
    "    device=device,\n",
    "    qdrant_client=client,\n",
    "    collection_name=collection_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bae4b261",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Successfully generated prediction with confidence 0.5530\n",
      "INFO:__main__:Identified condition: ear infections with confidence: 0.5530\n",
      "INFO:__main__:Identified condition: ear infections with confidence: 0.5530\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing query: Something grows on one of my dog ears\n",
      "\n",
      "Classification Results:\n",
      "Identified Condition: ear infections\n",
      "Confidence Score: 0.5530\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fhoss\\AppData\\Local\\Temp\\ipykernel_24240\\3655139355.py:79: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  results = self.client.search(\n",
      "INFO:httpx:HTTP Request: POST http://localhost:6333/collections/vet_notes/points/search \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Found 3 similar cases for condition: ear infections\n",
      "INFO:__main__:Found 3 similar cases for condition: ear infections\n",
      "INFO:__main__:Successfully generated prediction with confidence 0.8247\n",
      "INFO:__main__:Identified condition: parasites with confidence: 0.8247\n",
      "INFO:__main__:Successfully generated prediction with confidence 0.8247\n",
      "INFO:__main__:Identified condition: parasites with confidence: 0.8247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar Cases for ear infections:\n",
      "Score: 0.8421\n",
      "Case: evaluate conformation of ear canal (stenotic breeds).\n",
      "Condition: ear infections\n",
      "\n",
      "Score: 0.8395\n",
      "Case: use tris-edta containing ear flush for pseudomonas infections.\n",
      "Condition: ear infections\n",
      "\n",
      "Score: 0.8367\n",
      "Case: use ear wick or packing with medication for severe stenosis.\n",
      "Condition: ear infections\n",
      "\n",
      "Processing query: My cat has been scratching a lot and has some red spots\n",
      "\n",
      "Classification Results:\n",
      "Identified Condition: parasites\n",
      "Confidence Score: 0.8247\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:6333/collections/vet_notes/points/search \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Found 3 similar cases for condition: parasites\n",
      "INFO:__main__:Found 3 similar cases for condition: parasites\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar Cases for parasites:\n",
      "Score: 0.8267\n",
      "Case: ear mites in canal; clean and apply selamectin.\n",
      "Condition: parasites\n",
      "\n",
      "Score: 0.8093\n",
      "Case: sarcoptic mange; prescribe oral ivermectin.\n",
      "Condition: parasites\n",
      "\n",
      "Score: 0.8061\n",
      "Case: flea dirt present on coat combing; ctenocephalides felis infestation confirmed.\n",
      "Condition: parasites\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage with the agent system\n",
    "def process_user_query(user_input: str):\n",
    "    \"\"\"Process user query through both agents\"\"\"\n",
    "    print(f\"Processing query: {user_input}\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Use Classification Agent to identify the condition\n",
    "        condition, confidence = classification_agent.identify_condition(user_input)\n",
    "        print(f\"Classification Results:\")\n",
    "        print(f\"Identified Condition: {condition}\")\n",
    "        print(f\"Confidence Score: {confidence:.4f}\\n\")\n",
    "        \n",
    "        # Step 2: Use Retrieval Agent to find similar cases\n",
    "        similar_cases = retrieval_agent.find_similar_cases(user_input, condition)\n",
    "        \n",
    "        print(f\"Similar Cases for {condition}:\")\n",
    "        for case in similar_cases:\n",
    "            print(f\"Score: {case.score:.4f}\")\n",
    "            print(f\"Case: {case.payload['text']}\")\n",
    "            print(f\"Condition: {case.payload['condition']}\\n\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing query: {str(e)}\")\n",
    "\n",
    "# Test the system\n",
    "user_input = \"Something grows on one of my dog ears\"\n",
    "process_user_query(user_input)\n",
    "\n",
    "# Try another example\n",
    "user_input2 = \"My cat has been scratching a lot and has some red spots\"\n",
    "process_user_query(user_input2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a3bebe32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fhoss\\AppData\\Local\\Temp\\ipykernel_24240\\613116975.py:7: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  results = client.search(\n",
      "INFO:httpx:HTTP Request: POST http://localhost:6333/collections/vet_notes/points/search \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:6333/collections/vet_notes/points/search \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.8421, Text: evaluate conformation of ear canal (stenotic breeds)., Category: ear infections\n",
      "Score: 0.8395, Text: use tris-edta containing ear flush for pseudomonas infections., Category: ear infections\n",
      "Score: 0.8367, Text: use ear wick or packing with medication for severe stenosis., Category: ear infections\n"
     ]
    }
   ],
   "source": [
    "# Query example: find top 3 similar notes to a query vector, filtering by category \"dog\"\n",
    "# query_vector = [...]  # embedding vector for owner input\n",
    "filter_by_category = Filter(\n",
    "    must=[FieldCondition(key=\"condition\", match=MatchValue(value=\"ear infections\"))]\n",
    ")\n",
    "\n",
    "results = client.search(\n",
    "    collection_name=collection_name,\n",
    "    query_vector=query_vector[0],\n",
    "    limit=3,\n",
    "    query_filter=filter_by_category,\n",
    "    with_payload=True,  # returns metadata like text and category\n",
    ")\n",
    "\n",
    "for result in results:\n",
    "    print(f\"Score: {result.score:.4f}, Text: {result.payload['text']}, Category: {result.payload['condition']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fa9109",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
