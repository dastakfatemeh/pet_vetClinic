{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7072ab2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fhoss\\Documents\\jupyter\\Github\\pet_vetClinic\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "CUDA extension not installed.\n",
      "CUDA extension not installed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from qdrant_client import QdrantClient\n",
    "from transformers import pipeline\n",
    "from typing import Optional, Union, List, Tuple\n",
    "import torch.nn.functional as F\n",
    "import logging\n",
    "from qdrant_client import QdrantClient, models\n",
    "from qdrant_client.http.models import VectorParams, Distance, PointStruct, Filter, FieldCondition, MatchValue\n",
    "from huggingface_hub import login\n",
    "from HF_t import hf_token_read\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification,T5Tokenizer,T5ForConditionalGeneration,AutoModelForSeq2SeqLM\n",
    "import sentencepiece as spm\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "import re\n",
    "\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ad6ed1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device if not provided\n",
    "device = None\n",
    "if device is None:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf04548",
   "metadata": {},
   "source": [
    "# Connect to the Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c85d2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://localhost:6333 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:6333/collections/vet_notes \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "# Initilizae the Qdrant\n",
    "from qdrant_client import QdrantClient, models\n",
    "from qdrant_client.http.models import VectorParams, Distance, PointStruct, Filter, FieldCondition, MatchValue\n",
    "\n",
    "#initialize with a local path to persist data on disk without a server:\n",
    "#client = QdrantClient('./PetHealth_Chatbot/Vector_Database')\n",
    "# Connect to a local Qdrant instance\n",
    "client = QdrantClient(url=\"http://localhost:6333\") # Or specify your actual local host/port\n",
    "\n",
    "collection_name = \"vet_notes\"\n",
    "\n",
    "collection_info = client.get_collection(collection_name=collection_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d47628",
   "metadata": {},
   "source": [
    "# Loading Models\n",
    "First, we'll load two different models:\n",
    "1. VetBERT model for generating embeddings (vector representations)\n",
    "2. Classification model for predicting conditions\n",
    "3. Generating tex model for Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f6ec476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading VetBERT model for embeddings...\n",
      "VetBERT model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# 1. Load VetBERT model for embeddings\n",
    "print(\"Loading VetBERT model for embeddings...\")\n",
    "vetbert_model = AutoModel.from_pretrained(\"havocy28/VetBERT\")\n",
    "vetbert_tokenizer = AutoTokenizer.from_pretrained(\"havocy28/VetBERT\")\n",
    "\n",
    "# Check if a GPU is available and move the model to the GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# vetbert_model.to(device)\n",
    "print(\"VetBERT model loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bc0e114",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. loading the model from hugging face that trained on owner notes for classification\n",
    "#Use the token for reading to load the model in huging face\n",
    "from HF_t import hf_token_read\n",
    "\n",
    "# In your notebook\n",
    "# from huggingface_hub import login\n",
    "# from HF_t import hf_token_read  # Import your token securely\n",
    "\n",
    "login(token=hf_token_read)\n",
    "\n",
    "# Replace \"your-username/my-awesome-fine-tuned-model\" with your actual repository ID\n",
    "repo_id = \"fdastak/model_calssification\" \n",
    "\n",
    "# Load the fine-tuned model and its tokenizer\n",
    "model_Class= AutoModelForSequenceClassification.from_pretrained(repo_id)\n",
    "tokenizer_class = AutoTokenizer.from_pretrained(repo_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6a9c926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #3. Loading the model for summarization...\n",
    "# print(\"Loading ClinicalT5-base model for summarization...\")\n",
    "# # Load tokenizer and model from Hugging Face hub\n",
    "# # Load small T5 model (t5-small or t5-base)\n",
    "# model_name = \"t5-small\"\n",
    "\n",
    "# tokenizer_summ = T5Tokenizer.from_pretrained(model_name)\n",
    "# model_summ = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9e0715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading flan-t5-base model for expaination...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "#3. Loading the model for explaination...\n",
    "print(\"Loading flan-t5-base model for expaination...\")\n",
    "# Load tokenizer and model from Hugging Face hub\n",
    "# Load the model (flan-t5-base)\n",
    "model_name = \"google/flan-t5-base\"  # or flan-t5-large for better performance\n",
    "tokenizer_textg = AutoTokenizer.from_pretrained(model_name)\n",
    "model_textg = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "tokenizer_textg = T5Tokenizer.from_pretrained(model_name)\n",
    "model_textg = T5ForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ad07db",
   "metadata": {},
   "source": [
    "# Create an embeding class that can be used by differnt AI agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af7771a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mixin with the shared embedding method\n",
    "class VetBERTMixin:\n",
    "    def get_vetbert_embeddings(\n",
    "        self, \n",
    "        user_input: str, \n",
    "        return_numpy: bool = True\n",
    "    ) -> Union[torch.Tensor, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Generate embeddings using VetBERT model.\n",
    "\n",
    "        Args:\n",
    "            user_input (str): Text to embed\n",
    "            return_numpy (bool): Return numpy array if True, else torch tensor\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor or np.ndarray: Embedding vector\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        inputs = self.tokenizer(\n",
    "            user_input,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        )\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            embeddings = outputs.last_hidden_state[:, 0, :]  # CLS token\n",
    "\n",
    "        if return_numpy:\n",
    "            embeddings = embeddings.cpu().numpy()\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb5ec88",
   "metadata": {},
   "source": [
    "# Agent System\n",
    "\n",
    "We'll create two agents that work together:\n",
    "1. **Classification Agent**: Identifies the pet's condition from user input\n",
    "2. **Retrieval Agent**: Finds relevant veterinary notes based on the identified condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d435c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationAgent:\n",
    "    def __init__(self, model, tokenizer, device):\n",
    "        self.device = device\n",
    "        self.model = model.to(self.device) # Move model to device\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label_to_condition = {\n",
    "            0: \"digestive issues\",\n",
    "            1: \"ear infections\",\n",
    "            2: \"mobility problems\",\n",
    "            3: \"parasites\",\n",
    "            4: \"skin irritations\"\n",
    "        }\n",
    "\n",
    "    def predict_condition(self,user_input) -> tuple:\n",
    "        \"\"\"\n",
    "        Predict the condition for a given input text using the classification model.\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (predicted_label, confidence_score)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Set to eval mode\n",
    "            self.model.eval()\n",
    "            \n",
    "            # Tokenize input for classifier\n",
    "            inputs = self.tokenizer(\n",
    "                user_input,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=512\n",
    "            )\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "            \n",
    "            # Get predictions\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "                logits = outputs.logits\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                confidence_score, pred_label = torch.max(probs, dim=1)\n",
    "                \n",
    "                logger.info(f\"Successfully generated prediction with confidence {confidence_score.item():.4f}\")\n",
    "                return pred_label.item(), confidence_score.item()\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Prediction failed: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def identify_condition(self, user_input: str) -> tuple:\n",
    "        \"\"\"\n",
    "        Identify pet's condition from user input.\n",
    "            \n",
    "        Args:\n",
    "        user_input (str): Description of pet's symptoms\n",
    "                \n",
    "        Returns:\n",
    "        tuple: (condition_name, confidence_score)\n",
    "            \"\"\"\n",
    "        try:\n",
    "            # Get prediction\n",
    "            predicted_label, confidence = self.predict_condition(user_input)\n",
    "                \n",
    "            # Convert to condition name\n",
    "            condition_name = self.label_to_condition.get(predicted_label, \"unknown condition\")\n",
    "                \n",
    "            logger.info(f\"Identified condition: {condition_name} with confidence: {confidence:.4f}\")\n",
    "            return condition_name, confidence\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Error in classification: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "class RetrievalAgent(VetBERTMixin):\n",
    "    def __init__(self, model, tokenizer, device, qdrant_client, collection_name):\n",
    "        self.device = device\n",
    "        self.model = model.to(self.device) # Move model to device\n",
    "        self.tokenizer = tokenizer\n",
    "        self.client = qdrant_client\n",
    "        self.collection_name = collection_name\n",
    "\n",
    "    def find_similar_cases(self, user_input: str, condition: str, limit: int = 3) -> list:\n",
    "        \"\"\"\n",
    "        Find similar veterinary cases based on input and condition.\n",
    "        \n",
    "        Args:\n",
    "            user_input (str): User's description of symptoms\n",
    "            condition (str): Identified condition to filter by\n",
    "            limit (int): Number of similar cases to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            list: Similar cases with their scores\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            # Generate embeddings for the query\n",
    "            query_vector = self.get_vetbert_embeddings(\n",
    "                user_input,\n",
    "                return_numpy=True\n",
    "            )\n",
    "            \n",
    "            # Set up condition filter\n",
    "            condition_filter = Filter(\n",
    "                must=[FieldCondition(key=\"condition\", match=MatchValue(value=condition))]\n",
    "            )\n",
    "            \n",
    "            # Search for similar cases\n",
    "            results = self.client.search(\n",
    "                collection_name=self.collection_name,\n",
    "                query_vector=query_vector[0],\n",
    "                limit=limit,\n",
    "                query_filter=condition_filter,\n",
    "                with_payload=True\n",
    "            )\n",
    "            # qp = client.query_points(\n",
    "            # collection_name=collection_name,\n",
    "            # query=query_vector[0].tolist(),\n",
    "            # limit=3,\n",
    "            # query_filter=filter_by_category,\n",
    "            # with_payload=True,\n",
    "            #  )\n",
    "            # results = qp.points\n",
    "\n",
    "            \n",
    "            logger.info(f\"Found {len(results)} similar cases for condition: {condition}\")\n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Error in retrieval: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "class communicationAgent():\n",
    "    def __init__(self,model,tokenizer,device):\n",
    "        self.device = device\n",
    "        self.model = model.to(self.device) # Move model to device\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def output_vet_assistant(self,case):\n",
    "        # Prepare the input text for T5\n",
    "        finding_s=''\n",
    "        for i in range(len(case)):\n",
    "            \n",
    "        # 4. Create an effective prompt for the task\n",
    "        # The prompt should be clear and instructive, asking the model to perform two steps:\n",
    "        # a) Identify the clinical terms.\n",
    "        # b) Explain each term.\n",
    "            prompt_template = \"\"\"\n",
    "                Identify and explain clinical entities in the text provided.And provide a short, clear explanation for each term.\n",
    "                example :Gastroenteritis: inflammation of the stomach and intestines causing vomiting and diarrhea.\n",
    "                Text: {clinical_text}\"\"\"\n",
    "\n",
    "\n",
    "            prompt = prompt_template.format(clinical_text=case[i].payload['text'])\n",
    "            inputs= self.tokenizer(prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "            inputs= {k: v.to(device) for k, v in inputs.items()}\n",
    "            # Generate the output\n",
    "            outputs = self.model.generate(**inputs, max_new_tokens=50,  do_sample=True, \n",
    "                temperature=0.5,\n",
    "                top_p=0.9)\n",
    "            # Decode and print the output\n",
    "            decoded_output = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            finding_s=f\"{len(case)-i}.{case[i].payload['text']}\\n\"+f\"{decoded_output}\\n\"+finding_s\n",
    "        \n",
    "      \n",
    "\n",
    "        # Carefully craft a conversational question\n",
    "        appointment_question = (\n",
    "            \"\\n\\nWould you like to schedule an appointment with our veterinary team \"\n",
    "            \"to address these health findings and discuss the most effective treatment options for your pet?\"\n",
    "        )\n",
    "        \n",
    "        # Combine summary and question\n",
    "        final_output = final_output = (\n",
    "            f\"Here are my findings: (Sypmtoms,treatment, clinical explaination)\\n\\n\"\n",
    "            f\"{finding_s}\"\n",
    "            f\"{appointment_question}\"\n",
    "             )\n",
    "        print(final_output)\n",
    "        def deduplicate_numbered_sections(text):\n",
    "            # Split text into numbered blocks using regex\n",
    "            blocks = re.split(r'(?=\\d+\\.)', text)\n",
    "            seen = set()\n",
    "            unique_blocks = []\n",
    "            \n",
    "            for block in blocks:\n",
    "                cleaned_block = block.strip()\n",
    "                if cleaned_block and cleaned_block not in seen:\n",
    "                    unique_blocks.append(cleaned_block)\n",
    "                    seen.add(cleaned_block)\n",
    "            \n",
    "            # Join back with line breaks to keep original format\n",
    "            return '\\n'.join(unique_blocks)\n",
    "        final_output_dedup = deduplicate_numbered_sections(final_output)\n",
    "        return final_output_dedup\n",
    "\n",
    "# Initialize agents\n",
    "classification_agent = ClassificationAgent(\n",
    "    model=model_Class,\n",
    "    tokenizer=tokenizer_class,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "retrieval_agent = RetrievalAgent(\n",
    "    model=vetbert_model,\n",
    "    tokenizer=vetbert_tokenizer,\n",
    "    device=device,\n",
    "    qdrant_client=client,\n",
    "    collection_name=collection_name\n",
    ")\n",
    "\n",
    "communicationAgent = communicationAgent(\n",
    "    model=model_textg,\n",
    "    tokenizer=tokenizer_textg,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bae4b261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage with the agent system\n",
    "def process_user_query(user_input: str):\n",
    "    \"\"\"Process user query through both agents\"\"\"\n",
    "    print(f\"Processing query: {user_input}\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Use Classification Agent to identify the condition\n",
    "        condition, confidence = classification_agent.identify_condition(user_input)\n",
    "        # print(f\"Classification Results:\")\n",
    "        # print(f\"Identified Condition: {condition}\")\n",
    "        # print(f\"Confidence Score: {confidence:.4f}\\n\")\n",
    "        \n",
    "        # Step 2: Use Retrieval Agent to find similar cases\n",
    "        similar_cases = retrieval_agent.find_similar_cases(user_input, condition)\n",
    "        \n",
    "        # print(f\"Similar Cases for {condition}:\")\n",
    "        # for case in similar_cases:\n",
    "        #     print(f\"Score: {case.score:.4f}\")\n",
    "        #     print(f\"Case: {case.payload['text']}\")\n",
    "        #     print(f\"Condition: {case.payload['condition']}\\n\")\n",
    "\n",
    "        #step 3 explain the sitution in a simple words using communicationAgent\n",
    "        # Carefully craft a conversational question\n",
    "        explaination = communicationAgent.output_vet_assistant( similar_cases)\n",
    "        # print(f\"explaination:\\n{explaination}\\n\")\n",
    "        return explaination\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing query: {str(e)}\")\n",
    "\n",
    "# Test the system\n",
    "#user_input = \"Something grows on one of my dog ears\"\n",
    "#process_user_query(user_input)\n",
    "\n",
    "#Try another example\n",
    "# user_input2 = \"My cat has been scratching a lot and has some red spots\"\n",
    "# process_user_query(user_input2)\n",
    "\n",
    "# Another example\n",
    "# user_input3 = \"My dog has been vomiting and has diarrhea\"\n",
    "# process_user_query(user_input3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17edb768",
   "metadata": {},
   "source": [
    "# API Conversation System "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a35c606",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Define the request schema\n",
    "class SymptomInput(BaseModel):\n",
    "    user_input: str\n",
    "\n",
    "@app.post(\"/converse\")\n",
    "async def converse(input_data: SymptomInput):\n",
    "    user_input = input_data.user_input\n",
    "\n",
    "    try:\n",
    "        # Step 1: Classification to identify condition\n",
    "        condition_name, confidence = classification_agent.identify_condition(user_input)\n",
    "\n",
    "        # Step 2: Retrieve similar cases based on condition\n",
    "        similar_cases = retrieval_agent.find_similar_cases(user_input, condition_name)\n",
    "\n",
    "        # Step 3: Generate conversational summary and explanation\n",
    "        conversation_output = communicationAgent.output_vet_assistant(similar_cases)\n",
    "\n",
    "        # Compose full response\n",
    "        response = {\n",
    "            \"condition_identified\": condition_name,\n",
    "            \"confidence_score\": confidence,\n",
    "            \"similar_cases_count\": len(similar_cases),\n",
    "            \"conversation\": conversation_output\n",
    "        }\n",
    "        return response\n",
    "\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fa9109",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b2bd48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
