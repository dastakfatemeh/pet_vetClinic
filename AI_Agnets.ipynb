{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7072ab2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from qdrant_client import QdrantClient\n",
    "from transformers import pipeline\n",
    "from typing import Optional, Union, List, Tuple\n",
    "import torch.nn.functional as F\n",
    "import logging\n",
    "from qdrant_client import QdrantClient, models\n",
    "from qdrant_client.http.models import VectorParams, Distance, PointStruct, Filter, FieldCondition, MatchValue\n",
    "from huggingface_hub import login\n",
    "from HF_t import hf_token_read\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification,T5Tokenizer,T5ForConditionalGeneration,AutoModelForSeq2SeqLM\n",
    "import sentencepiece as spm\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "import re\n",
    "\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ad6ed1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device if not provided\n",
    "device = None\n",
    "if device is None:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf04548",
   "metadata": {},
   "source": [
    "# Connect to the Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c85d2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://localhost:6333 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:6333/collections/vet_notes \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "# Initilizae the Qdrant\n",
    "from qdrant_client import QdrantClient, models\n",
    "from qdrant_client.http.models import VectorParams, Distance, PointStruct, Filter, FieldCondition, MatchValue\n",
    "\n",
    "#initialize with a local path to persist data on disk without a server:\n",
    "#client = QdrantClient('./PetHealth_Chatbot/Vector_Database')\n",
    "# Connect to a local Qdrant instance\n",
    "client = QdrantClient(url=\"http://localhost:6333\") # Or specify your actual local host/port\n",
    "\n",
    "collection_name = \"vet_notes\"\n",
    "\n",
    "collection_info = client.get_collection(collection_name=collection_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d47628",
   "metadata": {},
   "source": [
    "# Loading Models\n",
    "First, we'll load two different models:\n",
    "1. VetBERT model for generating embeddings (vector representations)\n",
    "2. Classification model for predicting conditions\n",
    "3. Generating tex model for Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f6ec476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading VetBERT model for embeddings...\n",
      "VetBERT model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# 1. Load VetBERT model for embeddings\n",
    "print(\"Loading VetBERT model for embeddings...\")\n",
    "vetbert_model = AutoModel.from_pretrained(\"havocy28/VetBERT\")\n",
    "vetbert_tokenizer = AutoTokenizer.from_pretrained(\"havocy28/VetBERT\")\n",
    "\n",
    "# Check if a GPU is available and move the model to the GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# vetbert_model.to(device)\n",
    "print(\"VetBERT model loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bc0e114",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. loading the model from hugging face that trained on owner notes for classification\n",
    "#Use the token for reading to load the model in huging face\n",
    "from HF_t import hf_token_read\n",
    "\n",
    "# In your notebook\n",
    "# from huggingface_hub import login\n",
    "# from HF_t import hf_token_read  # Import your token securely\n",
    "\n",
    "login(token=hf_token_read)\n",
    "\n",
    "# Replace \"your-username/my-awesome-fine-tuned-model\" with your actual repository ID\n",
    "repo_id = \"fdastak/model_calssification\" \n",
    "\n",
    "# Load the fine-tuned model and its tokenizer\n",
    "model_Class= AutoModelForSequenceClassification.from_pretrained(repo_id)\n",
    "tokenizer_class = AutoTokenizer.from_pretrained(repo_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c6a9c926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ClinicalT5-base model for summarization...\n"
     ]
    }
   ],
   "source": [
    "#3. Loading the model for summarization...\n",
    "print(\"Loading ClinicalT5-base model for summarization...\")\n",
    "# Load tokenizer and model from Hugging Face hub\n",
    "# Load small T5 model (t5-small or t5-base)\n",
    "model_name = \"t5-small\"\n",
    "\n",
    "tokenizer_summ = T5Tokenizer.from_pretrained(model_name)\n",
    "model_summ = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9e0715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ClinicalT5-base model for expaination...\n"
     ]
    }
   ],
   "source": [
    "#4. Loading the model for explaination...\n",
    "print(\"Loading flan-t5-base model for expaination...\")\n",
    "# Load tokenizer and model from Hugging Face hub\n",
    "# Load the model (flan-t5-base)\n",
    "model_name = \"google/flan-t5-base\"  # or flan-t5-large for better performance\n",
    "tokenizer_textg = AutoTokenizer.from_pretrained(model_name)\n",
    "model_textg = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "tokenizer_textg = T5Tokenizer.from_pretrained(model_name)\n",
    "model_textg = T5ForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ad07db",
   "metadata": {},
   "source": [
    "# Create an embeding class that can be used by differnt AI agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af7771a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mixin with the shared embedding method\n",
    "class VetBERTMixin:\n",
    "    def get_vetbert_embeddings(\n",
    "        self, \n",
    "        user_input: str, \n",
    "        return_numpy: bool = True\n",
    "    ) -> Union[torch.Tensor, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Generate embeddings using VetBERT model.\n",
    "\n",
    "        Args:\n",
    "            user_input (str): Text to embed\n",
    "            return_numpy (bool): Return numpy array if True, else torch tensor\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor or np.ndarray: Embedding vector\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        inputs = self.tokenizer(\n",
    "            user_input,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        )\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            embeddings = outputs.last_hidden_state[:, 0, :]  # CLS token\n",
    "\n",
    "        if return_numpy:\n",
    "            embeddings = embeddings.cpu().numpy()\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb5ec88",
   "metadata": {},
   "source": [
    "# Agent System\n",
    "\n",
    "We'll create two agents that work together:\n",
    "1. **Classification Agent**: Identifies the pet's condition from user input\n",
    "2. **Retrieval Agent**: Finds relevant veterinary notes based on the identified condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "0d435c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationAgent:\n",
    "    def __init__(self, model, tokenizer, device):\n",
    "        self.device = device\n",
    "        self.model = model.to(self.device) # Move model to device\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label_to_condition = {\n",
    "            0: \"digestive issues\",\n",
    "            1: \"ear infections\",\n",
    "            2: \"mobility problems\",\n",
    "            3: \"parasites\",\n",
    "            4: \"skin irritations\"\n",
    "        }\n",
    "\n",
    "    def predict_condition(self,user_input) -> tuple:\n",
    "        \"\"\"\n",
    "        Predict the condition for a given input text using the classification model.\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (predicted_label, confidence_score)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Set to eval mode\n",
    "            self.model.eval()\n",
    "            \n",
    "            # Tokenize input for classifier\n",
    "            inputs = self.tokenizer(\n",
    "                user_input,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=512\n",
    "            )\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "            \n",
    "            # Get predictions\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "                logits = outputs.logits\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                confidence_score, pred_label = torch.max(probs, dim=1)\n",
    "                \n",
    "                logger.info(f\"Successfully generated prediction with confidence {confidence_score.item():.4f}\")\n",
    "                return pred_label.item(), confidence_score.item()\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Prediction failed: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def identify_condition(self, user_input: str) -> tuple:\n",
    "        \"\"\"\n",
    "        Identify pet's condition from user input.\n",
    "            \n",
    "        Args:\n",
    "        user_input (str): Description of pet's symptoms\n",
    "                \n",
    "        Returns:\n",
    "        tuple: (condition_name, confidence_score)\n",
    "            \"\"\"\n",
    "        try:\n",
    "            # Get prediction\n",
    "            predicted_label, confidence = self.predict_condition(user_input)\n",
    "                \n",
    "            # Convert to condition name\n",
    "            condition_name = self.label_to_condition.get(predicted_label, \"unknown condition\")\n",
    "                \n",
    "            logger.info(f\"Identified condition: {condition_name} with confidence: {confidence:.4f}\")\n",
    "            return condition_name, confidence\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Error in classification: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "class RetrievalAgent(VetBERTMixin):\n",
    "    def __init__(self, model, tokenizer, device, qdrant_client, collection_name):\n",
    "        self.device = device\n",
    "        self.model = model.to(self.device) # Move model to device\n",
    "        self.tokenizer = tokenizer\n",
    "        self.client = qdrant_client\n",
    "        self.collection_name = collection_name\n",
    "\n",
    "    def find_similar_cases(self, user_input: str, condition: str, limit: int = 3) -> list:\n",
    "        \"\"\"\n",
    "        Find similar veterinary cases based on input and condition.\n",
    "        \n",
    "        Args:\n",
    "            user_input (str): User's description of symptoms\n",
    "            condition (str): Identified condition to filter by\n",
    "            limit (int): Number of similar cases to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            list: Similar cases with their scores\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            # Generate embeddings for the query\n",
    "            query_vector = self.get_vetbert_embeddings(\n",
    "                user_input,\n",
    "                return_numpy=True\n",
    "            )\n",
    "            \n",
    "            # Set up condition filter\n",
    "            condition_filter = Filter(\n",
    "                must=[FieldCondition(key=\"condition\", match=MatchValue(value=condition))]\n",
    "            )\n",
    "            \n",
    "            # Search for similar cases\n",
    "            results = self.client.search(\n",
    "                collection_name=self.collection_name,\n",
    "                query_vector=query_vector[0],\n",
    "                limit=limit,\n",
    "                query_filter=condition_filter,\n",
    "                with_payload=True\n",
    "            )\n",
    "            # qp = client.query_points(\n",
    "            # collection_name=collection_name,\n",
    "            # query=query_vector[0].tolist(),\n",
    "            # limit=3,\n",
    "            # query_filter=filter_by_category,\n",
    "            # with_payload=True,\n",
    "            #  )\n",
    "            # results = qp.points\n",
    "\n",
    "            \n",
    "            logger.info(f\"Found {len(results)} similar cases for condition: {condition}\")\n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Error in retrieval: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "class communicationAgent():\n",
    "    def __init__(self,model,tokenizer,device):\n",
    "        self.device = device\n",
    "        self.model = model.to(self.device) # Move model to device\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def output_vet_assistant(self,case):\n",
    "        # Prepare the input text for T5\n",
    "        finding_s=''\n",
    "        for i in range(len(case)):\n",
    "            \n",
    "        # 4. Create an effective prompt for the task\n",
    "        # The prompt should be clear and instructive, asking the model to perform two steps:\n",
    "        # a) Identify the clinical terms.\n",
    "        # b) Explain each term.\n",
    "            prompt_template = \"\"\"\n",
    "                Identify and explain clinical entities in the text provided.And provide a short, clear explanation for each term.\n",
    "                example :Gastroenteritis: inflammation of the stomach and intestines causing vomiting and diarrhea.\n",
    "                Text: {clinical_text}\"\"\"\n",
    "\n",
    "\n",
    "            prompt = prompt_template.format(clinical_text=case[i].payload['text'])\n",
    "            inputs= self.tokenizer(prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "            inputs= {k: v.to(device) for k, v in inputs.items()}\n",
    "            # Generate the output\n",
    "            outputs = self.model.generate(**inputs, max_new_tokens=50,  do_sample=True, \n",
    "                temperature=0.5,\n",
    "                top_p=0.9)\n",
    "            # Decode and print the output\n",
    "            decoded_output = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            finding_s=f\"{len(case)-i}.{case[i].payload['text']}\\n\"+f\"{decoded_output}\\n\"+finding_s\n",
    "        \n",
    "      \n",
    "\n",
    "        # Carefully craft a conversational question\n",
    "        appointment_question = (\n",
    "            \"\\n\\nWould you like to schedule an appointment with our veterinary team \"\n",
    "            \"to address these health findings and discuss the most effective treatment options for your pet?\"\n",
    "        )\n",
    "        \n",
    "        # Combine summary and question\n",
    "        final_output = final_output = (\n",
    "            f\"Here are my findings: (Sypmtoms,treatment, clinical explaination)\\n\\n\"\n",
    "            f\"{finding_s}\"\n",
    "            f\"{appointment_question}\"\n",
    "             )\n",
    "        print(final_output)\n",
    "        def deduplicate_numbered_sections(text):\n",
    "            # Split text into numbered blocks using regex\n",
    "            blocks = re.split(r'(?=\\d+\\.)', text)\n",
    "            seen = set()\n",
    "            unique_blocks = []\n",
    "            \n",
    "            for block in blocks:\n",
    "                cleaned_block = block.strip()\n",
    "                if cleaned_block and cleaned_block not in seen:\n",
    "                    unique_blocks.append(cleaned_block)\n",
    "                    seen.add(cleaned_block)\n",
    "            \n",
    "            # Join back with line breaks to keep original format\n",
    "            return '\\n'.join(unique_blocks)\n",
    "        final_output_dedup = deduplicate_numbered_sections(final_output)\n",
    "        return final_output_dedup\n",
    "\n",
    "# Initialize agents\n",
    "classification_agent = ClassificationAgent(\n",
    "    model=model_Class,\n",
    "    tokenizer=tokenizer_class,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "retrieval_agent = RetrievalAgent(\n",
    "    model=vetbert_model,\n",
    "    tokenizer=vetbert_tokenizer,\n",
    "    device=device,\n",
    "    qdrant_client=client,\n",
    "    collection_name=collection_name\n",
    ")\n",
    "\n",
    "communicationAgent = communicationAgent(\n",
    "    model=model_textg,\n",
    "    tokenizer=tokenizer_textg,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "bae4b261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing query: My cat has been scratching a lot and has some red spots\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Successfully generated prediction with confidence 0.8247\n",
      "INFO:__main__:Identified condition: parasites with confidence: 0.8247\n",
      "C:\\Users\\fhoss\\AppData\\Local\\Temp\\ipykernel_31184\\3267297700.py:107: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  results = self.client.search(\n",
      "INFO:httpx:HTTP Request: POST http://localhost:6333/collections/vet_notes/points/search \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Found 3 similar cases for condition: parasites\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are my findings: (Sypmtoms,treatment, clinical explaination)\n",
      "\n",
      "1.flea dirt present on coat combing; ctenocephalides felis infestation confirmed.\n",
      "ctenocephalides felis is a species of flies in the family Ctenocephalides.\n",
      "2.sarcoptic mange; prescribe oral ivermectin.\n",
      "sarcoptic mange is a cutaneous condition characterized by a rash, rash, and a rash that is characterized by a rash and rash that is characterized by a rash\n",
      "3.ear mites in canal; clean and apply selamectin.\n",
      "Selamectin is an antibiotic that can be used to treat ear mites in the canal.\n",
      "\n",
      "\n",
      "Would you like to schedule an appointment with our veterinary team to address these health findings and discuss the most effective treatment options for your pet?\n",
      "explaination:\n",
      "Here are my findings: (Sypmtoms,treatment, clinical explaination)\n",
      "1.flea dirt present on coat combing; ctenocephalides felis infestation confirmed.\n",
      "ctenocephalides felis is a species of flies in the family Ctenocephalides.\n",
      "2.sarcoptic mange; prescribe oral ivermectin.\n",
      "sarcoptic mange is a cutaneous condition characterized by a rash, rash, and a rash that is characterized by a rash and rash that is characterized by a rash\n",
      "3.ear mites in canal; clean and apply selamectin.\n",
      "Selamectin is an antibiotic that can be used to treat ear mites in the canal.\n",
      "\n",
      "\n",
      "Would you like to schedule an appointment with our veterinary team to address these health findings and discuss the most effective treatment options for your pet?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage with the agent system\n",
    "def process_user_query(user_input: str):\n",
    "    \"\"\"Process user query through both agents\"\"\"\n",
    "    print(f\"Processing query: {user_input}\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Use Classification Agent to identify the condition\n",
    "        condition, confidence = classification_agent.identify_condition(user_input)\n",
    "        # print(f\"Classification Results:\")\n",
    "        # print(f\"Identified Condition: {condition}\")\n",
    "        # print(f\"Confidence Score: {confidence:.4f}\\n\")\n",
    "        \n",
    "        # Step 2: Use Retrieval Agent to find similar cases\n",
    "        similar_cases = retrieval_agent.find_similar_cases(user_input, condition)\n",
    "        \n",
    "        # print(f\"Similar Cases for {condition}:\")\n",
    "        # for case in similar_cases:\n",
    "        #     print(f\"Score: {case.score:.4f}\")\n",
    "        #     print(f\"Case: {case.payload['text']}\")\n",
    "        #     print(f\"Condition: {case.payload['condition']}\\n\")\n",
    "\n",
    "        #step 3 explain the sitution in a simple words using communicationAgent\n",
    "        # Carefully craft a conversational question\n",
    "        explaination = communicationAgent.output_vet_assistant( similar_cases)\n",
    "        print(f\"explaination:\\n{explaination}\\n\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing query: {str(e)}\")\n",
    "\n",
    "# Test the system\n",
    "#user_input = \"Something grows on one of my dog ears\"\n",
    "#process_user_query(user_input)\n",
    "\n",
    "#Try another example\n",
    "user_input2 = \"My cat has been scratching a lot and has some red spots\"\n",
    "process_user_query(user_input2)\n",
    "\n",
    "# Another example\n",
    "# user_input3 = \"My dog has been vomiting and has diarrhea\"\n",
    "# process_user_query(user_input3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fa9109",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b2bd48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
